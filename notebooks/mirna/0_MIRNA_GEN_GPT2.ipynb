{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ad5528-1e2f-4282-a5d1-66a62ab13f1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83372c-17d6-4f86-9c87-6003a13bc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0bc518-69bb-433d-8cd9-e6f6de612594",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "DATA_PATH = os.path.join(ROOT_PATH, 'data')\n",
    "MIRNA_DATA_PATH = os.path.join(DATA_PATH, 'mirna.tsv')\n",
    "MIRNA_MODEL_NAME = \"mirna\"\n",
    "MIRNA_MODEL_VERSION = \"0\"\n",
    "MIRNA_MODEL_CHECKPOINT = \"checkpoint-1000\"\n",
    "MIRNA_MODEL_ROOT = os.path.join(os.getcwd(), f\"model={MIRNA_MODEL_NAME}\", f\"version={MIRNA_MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece5fd5-4792-4a52-9178-de0d1561cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(MIRNA_MODEL_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861cac1a-406a-4226-8e79-bfdfdb5faaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRNA_DATA_PATH, MIRNA_MODEL_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3570b-3fb2-4d9e-b0f5-6b09274dea43",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05f736-f89f-4a26-aa4f-425bfb959740",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_str = open(MIRNA_DATA_PATH).read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf981cb-d1cb-4d2c-8b20-027ccd0c1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_list = re.split(' |\\n', mirna_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e175ca2-1996-4a42-b755-83880cdbe543",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mirna_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92fb9ec-8733-4fc9-abb0-5333a83f707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_list[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de38aad-538a-4f9d-9463-18549ccd8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_list[-9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f7954-ee1f-4854-a012-2b24e1722d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_array_1 = np.array(mirna_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572750c-7642-451a-98c5-918ec707f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_array_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032b780-238d-4b5d-94a9-7c40f7c8e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_array_2 = mirna_array_1.reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583b788-27cf-4020-8757-273239b4eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = pd.DataFrame.from_records(mirna_array_2, columns=['ID', 'Accession', 'sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a5a83-c40c-4bc7-a440-2c2413e960b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma['ID'] = ma['ID'].map(lambda _: _[1:] if _.startswith('>') else _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2555d59-08aa-43f3-ab05-c2152a14df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b0d66-c46d-419d-a3d3-5c8cefc7d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce(lambda a, b: a.union(set(b)), ma.sequence, set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d632141-8e30-4504-b478-eb9208babfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ma.iloc[0].sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984161c1-7c31-4d25-8858-35ced6055cf0",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672fd4d4-ea79-44bb-977f-ab9877c109b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN_FRACTION = 9/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe274916-485b-4752-8711-969a9fcbf220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825dd0a3-6627-4040-b42d-d83ab65ae287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085e87b-eb0a-47bc-9e9b-8b4b932cb4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dataset.from_generator?\n",
    "mirna_train_dataset = Dataset.from_pandas(ma.iloc[:int(DATASET_TRAIN_FRACTION*len(ma))], split='train')\n",
    "mirna_test_dataset = Dataset.from_pandas(ma.iloc[int(DATASET_TRAIN_FRACTION*len(ma)):], split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5596ae-dabb-4151-9a62-f6426dc8864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(mirna_train_dataset.data.table, pa.Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb03f96-b4f0-4e8b-8fd1-e2a7cc2ed33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRNA_TRAIN_PATH = os.path.join(MIRNA_MODEL_ROOT, f\"{MIRNA_MODEL_NAME}_train.parquet\")\n",
    "pq.write_table(mirna_train_dataset.data.table, MIRNA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fb435-6a0b-4bb9-8e73-2c564a1f4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRNA_TEST_PATH = os.path.join(MIRNA_MODEL_ROOT, f\"{MIRNA_MODEL_NAME}_test.parquet\")\n",
    "pq.write_table(mirna_test_dataset.data.table, MIRNA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c5583-4f41-4255-b5b0-4966bd0793e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": mirna_train_dataset,  # .shuffle().select(range(50000)),\n",
    "        \"test\": mirna_test_dataset,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fef3c899-6b85-451b-9d71-865beb0fd62e",
   "metadata": {},
   "source": [
    "mirna_dataset_dict = load_dataset(\"parquet\", \n",
    "                                  data_files={splits.Split.TRAIN: MIRNA_TRAIN_PATH, splits.Split.TEST: MIRNA_TEST_PATH},)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55f1688b-f1cd-46a9-af4b-74e52c148d0c",
   "metadata": {},
   "source": [
    "mirna_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41806611-0bda-471e-a8e5-b4cc2263de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sequence0 = ma.iloc[0].sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe401e-1489-409d-a39d-0520219fe9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRNA_MAX_SEQUENCE_LENGTH = max(max(len(tt) for tt in ds['sequence']) for ds in mirna_datasets.values())\n",
    "MIRNA_MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d7206-907c-49d9-bc02-85f3fe5902db",
   "metadata": {},
   "source": [
    "# TOKENIZER\n",
    "\n",
    "Building following tutorial: https://huggingface.co/course/chapter6/8?fw=pt\n",
    "\n",
    "Specifically, 'GPT-2' BPE-based Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e97e2-25de-48a8-b827-010bb05f50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def get_tokenizer_training_corpus(dataset_dict, *, chunk_size):\n",
    "    dd = dataset_dict\n",
    "    sequence_list = reduce(lambda sequence, ds: sequence+ds['sequence'], dd.values(), [])\n",
    "    for i in range(0, len(sequence_list), chunk_size):\n",
    "        yield sequence_list[i:i+chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e303553-c356-48c8-87fd-e37695c5ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62f7a8fb-df27-4706-99cb-b4485063805e",
   "metadata": {},
   "source": [
    "#_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\", bos_token=\"[BOS]\", eos_token=\"[EOS]\"))\n",
    "_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ddc77-aa12-48f3-ad03-07c623b69de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = Tokenizer(models.BPE()) # all tokens are known"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e4f3b61-8907-4728-ae04-c35722add884",
   "metadata": {},
   "source": [
    "# No normalization is necessary\n",
    "_tokenizer.normalizer = normalizers.Sequence(\n",
    "    [#normalizers.NFD(), \n",
    "     normalizers.Lowercase(), \n",
    "     normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d0c8598-3a6a-4875-92e3-c30280ddb0c5",
   "metadata": {},
   "source": [
    "_tokenizer.normalizer.normalize_str(_sequence0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e78479b9-4ee8-41fc-bcc6-b74785bd4d11",
   "metadata": {},
   "source": [
    "_tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aedea761-0db2-4429-b0c9-771945806665",
   "metadata": {},
   "source": [
    "_tokenizer.pre_tokenizer.pre_tokenize_str(_tokenizer.normalizer.normalize_str(_sequence0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf37eea-f99d-46fd-bdfb-9fce7f3a5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44203d0e-fb88-4fa9-a8f3-86af5c3d8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer.pre_tokenizer.pre_tokenize_str(_sequence0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4c105-2c9e-4121-a1b1-de2fbd972002",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_TRAINER_CHUNK_SIZE = 200\n",
    "VOCAB_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e681d-e3c4-4a4d-a42d-0eabb096fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c49813-b2f9-4713-8c91-4632c7f156a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer.train_from_iterator(get_tokenizer_training_corpus(mirna_datasets, chunk_size=TOKENIZER_TRAINER_CHUNK_SIZE), \n",
    "                              trainer=tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f72365-5b20-4604-80d4-ffae929c39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239e12e-eecf-4e01-87d1-739eb5ca70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_encoding = _tokenizer.encode(_sequence0)\n",
    "_tokens = _encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710fe509-98e2-4a0e-ac3c-2f1ee53ead89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b67c7-72e0-495f-8a1d-556b1c031ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a755ca-cfbf-48e7-a1fd-3f5c5546ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_ = _tokenizer.encode(_sequence0)\n",
    "start, end = encoding_.offsets[4]\n",
    "_sequence0[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc1dd0-1fd5-4693-bb39-bbbc972cbce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe899cb-4aff-4303-93b9-1b5399072085",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence0_ = _tokenizer.decode(_encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9de4a-9d15-459e-9f4f-e2607a614f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sequence0 == sequence0_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ded74-36d1-446f-95a2-0ab76cbc620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = os.path.join(MIRNA_MODEL_ROOT, f\"{MIRNA_MODEL_NAME}_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30cfb6-5219-4e00-b90f-4f315b145b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer.save(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5acae7d-078e-4b1e-bf43-38f151d22230",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = Tokenizer.from_file(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf680e12-e1e5-4463-9458-6d7d03412d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=_tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224b175-c7c5-415a-919f-d9c890227189",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(_sequence0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072322d-debf-43b1-8a11-bc4ec1e5a9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c9659-2099-475c-805e-8db36069b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2da50a-b13c-4f22-9b56-1411742db5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef1660-a262-4974-90a5-2febacd2c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer(mirna_datasets['train']['sequence'])['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2109ca6-aaff-4347-bd3f-c624d3da267a",
   "metadata": {},
   "source": [
    "## Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb1321-bb6c-43ad-934b-a7542d92908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRNA_MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802d33f-2bbb-491a-9385-5100081d93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRNA_CONTEXT_LENGTH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad042e-a371-414d-aa6d-31eeef742b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27921acf-ab86-42e2-9513-932503ce7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row, *, context_length=MIRNA_CONTEXT_LENGTH):\n",
    "    outputs = tokenizer(\n",
    "        row[\"sequence\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    return outputs\n",
    "    \"\"\"\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "mirna_tokenized_datasets = mirna_datasets.map(tokenize, batched=True, remove_columns=mirna_datasets[\"train\"].column_names)\n",
    "mirna_tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19858b6-840b-4b5f-a6fb-6ec0ce37ce9f",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c302c-2ff7-4d16-ada7-2e5bb2f519b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=MIRNA_CONTEXT_LENGTH,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c23dd4-587e-4c89-b5f2-56d83eb80fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb89a46d-9d65-48bf-a1a9-449c48834051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae464b7-5873-49a9-81f4-79fe1ffe51a0",
   "metadata": {},
   "source": [
    "Let’s have a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa5282-2d36-4274-9221-51efe043da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = data_collator([mirna_tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a76d8-90ed-4b38-88f6-115204b8b94c",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2dbae-e80e-4e4b-8048-f2b075ba1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = 3\n",
    "# Set up CUDA environment BEFORE importing torch\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"TRUE\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{GPU}\"  # This shrinks the GPU universe and maps cuda:0 to {GPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf2acc-c647-4350-a664-232b1d300b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f6c0e-35a7-4d28-b20b-20c4add85629",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device() # This really is device {GPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72da025-416c-46d8-8952-60b8c5f275af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import datetime\n",
    "date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "time = datetime.datetime.now().strftime('%H.%M')\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MIRNA_MODEL_ROOT,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    #fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{MIRNA_MODEL_ROOT}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    #place_model_on_device=torch.device(f\"cuda:{GPU}\"),\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=12.0,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=mirna_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=mirna_tokenized_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77934629-9f8d-48af-b5d7-5b1070a2e44d",
   "metadata": {},
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc7b16-70e9-47af-9884-a9ef5b61b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = model.from_pretrained(MIRNA_MODEL_ROOT)\n",
    "except:\n",
    "    model.to(f\"cuda:0\")\n",
    "    trainer.train()\n",
    "    model.to(\"cpu\").save_pretrained(MIRNA_MODEL_ROOT, from_pt=True)\n",
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3376c-f303-4611-a7a4-4396ba2a94c0",
   "metadata": {},
   "source": [
    "### Quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56931f-2655-40ab-8d32-ad56df98e8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(\"\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc174d42-3def-4e09-82bd-f06e13ca39f1",
   "metadata": {},
   "source": [
    "# LATENT-SPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e556bb-4e81-4c39-acb5-9b48cac1610c",
   "metadata": {},
   "source": [
    "# #TODO:\n",
    "* UMAP landscape of latent space on MIRNA\n",
    "    - Extract lm_head LATENT INPUTS for each of the generated sequences:\n",
    "        x Construct suitable args/kwargs for GPT2LMHeadModel.generate(input_ids, attention_mask, **generate_kwargs)\n",
    "        - Compute latent codes for the training set sequences: \n",
    "            - Tokenize each sequence\n",
    "            - Evaluate model on all of each of the sequence's initial segments, collecting the appropriate hidden states\n",
    "            - Use training sequences long_latents and include them in UMAP\n",
    "                   \n",
    "    - UMAP the latent inputs\n",
    "* PERPLEXITY/CROSS-ENTROPY measure of training set sequences\n",
    "    - LITERATURE on how to evaluate the model performance by looking at perplexity\n",
    "    - CODE for how to compute perplexity from model generation/transition scores\n",
    "    - UNDERSTAND how CROSS-ENTROPY LOSS relates to PERPLEXITY\n",
    "* PRETRAIN on other RNA datasets\n",
    "* DECONFOUNDING/separation/whitening in latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b293e42-fbde-4711-9f22-90dbd63e2b19",
   "metadata": {},
   "source": [
    "From `transformers.generation.utils.GenerationMixin.compute_transition_scores()` docstring:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "...\n",
    "\n",
    "Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
    "        >>> import numpy as np\n",
    "\n",
    "        >>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "        >>> tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        >>> inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Example 1: Print the scores for each token generated with Greedy Search\n",
    "        >>> outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n",
    "        >>> transition_scores = model.compute_transition_scores(\n",
    "        ...     outputs.sequences, outputs.scores, normalize_logits=True\n",
    "        ... )\n",
    "        >>> input_length = inputs.input_ids.shape[1]\n",
    "        >>> generated_tokens = outputs.sequences[:, input_length:]\n",
    "        >>> for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "        ...     # | token | token string | logits | probability\n",
    "        ...     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n",
    "        |   262 |  the     | -1.414 | 24.33%\n",
    "        |  1110 |  day     | -2.609 | 7.36%\n",
    "        |   618 |  when    | -2.010 | 13.40%\n",
    "        |   356 |  we      | -1.859 | 15.58%\n",
    "        |   460 |  can     | -2.508 | 8.14%\n",
    "\n",
    "        >>> # Example 2: Reconstruct the sequence scores from Beam Search\n",
    "        >>> outputs = model.generate(\n",
    "        ...     **inputs,\n",
    "        ...     max_new_tokens=5,\n",
    "        ...     num_beams=4,\n",
    "        ...     num_return_sequences=4,\n",
    "        ...     return_dict_in_generate=True,\n",
    "        ...     output_scores=True,\n",
    "        ... )\n",
    "        >>> transition_scores = model.compute_transition_scores(\n",
    "        ...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
    "        ... )\n",
    "        >>> # If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
    "        >>> # Tip: set `normalize_logits=True` to recompute the scores from the normalized logits.\n",
    "        >>> output_length = inputs.input_ids.shape[1] + np.sum(transition_scores.numpy() < 0, axis=1)\n",
    "        >>> length_penalty = model.generation_config.length_penalty\n",
    "        >>> reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
    "        >>> print(np.allclose(outputs.sequences_scores, reconstructed_scores))\n",
    "        True\n",
    "        ```\n",
    "\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81da89c9-9ef1-4f9b-a9e6-6cd1a3ef5332",
   "metadata": {},
   "source": [
    "`generator(\"\", max_length=30, num_return_sequences=5)` results in this call to `GenerationMixin.generate()`:\n",
    "\n",
    "`model.generate(input_ids=None, attention_mask=None, generate_kwargs={'max_length': 30, 'num_return_sequences': 5})`\n",
    "\n",
    "We augment it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f26060-662f-4855-9e1c-255e7edf76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RETURN_SEQUENCES = 100\n",
    "MAX_LENGTH = MIRNA_MAX_SEQUENCE_LENGTH\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a5394-a0fe-4f9d-af7f-ea555968c92e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "outputs = model.generate(input_ids=None, \n",
    "                         attention_mask=None, \n",
    "                         return_dict_in_generate=True, \n",
    "                         output_scores=True,        # to compute perplexity/cross-entropy later\n",
    "                         output_attentions=True,    # for viz\n",
    "                         output_hidden_states=True, # for UMAP\n",
    "                         max_length=MAX_LENGTH, \n",
    "                         num_return_sequences=NUM_RETURN_SEQUENCES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70626c0b-5086-4213-8260-2aad7b6ea933",
   "metadata": {},
   "source": [
    "## SEQUENCES (HEAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a56e6-ad96-4a17-8588-c9fb81302dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 5\n",
    "tokenizer.batch_decode(outputs.sequences[:N_SAMPLES,:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2caf12-9691-4b07-b1e4-7a4027a9a6fc",
   "metadata": {},
   "source": [
    "## LATENTS (UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ec24a-b00f-402b-8480-e14b691f89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa62c0f-7d96-4ce6-921d-050685344dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs.hidden_states) # one per sequence element, except the last one -- the model is not evaluated on it as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997de997-9583-4645-b9b1-660835b543bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs.hidden_states[0]) # one per layer: 12 GPT2Blocks followd by a LayerNorm for a total of 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed06bc3-fbb5-4c9d-b9c5-2c97cb980d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states[0][-1].shape # last layer is the logits -- the activations of the final LayerNorm following the 12 transformer blocks\n",
    "# shape: [NUM_RETURN_SEQUENCES, 1, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c5bec-1b25-41aa-9de4-fe7ec0c49e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_latents: concat the activations from the last hidden layer for all sequence elements\n",
    "# H is the last activation of shape [N, 1, D]\n",
    "long_latents_list = [h.reshape(h.shape[0], h.shape[-1]) for h in [H[-1] for H in outputs.hidden_states]]\n",
    "long_latents = torch.cat(long_latents_list, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68caad-94e1-4711-8b55-7cc4f3733dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab10006-ef5f-43ff-a365-8a119e660b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_latents: take the last hidden layer activation for the last element of each sequence\n",
    "_ = outputs.hidden_states[-1][-1]\n",
    "short_latents = _.reshape((_.shape[0], _.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42841f23-c46b-4464-8e1f-dc963156d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_latents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1f1c5-c18b-4747-866d-023421afd16c",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf04e26-76ef-4e7f-b26a-a38be78e14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430543dd-c709-4b45-9970-45f139df8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='poster', rc={'figure.figsize':(1000,500)})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d30de2-6502-4ad4-85a0-5fd1ac8f2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41efa952-81de-4672-bde3-2ebaf231ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lu = fit.fit_transform(long_latents.cpu().detach().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c32e14-3eba-489f-81fe-b7a0a74a62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b53dc2-97ef-4c7b-a289-885e2f1bd53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=lu[:,0], y=lu[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ed2e5-109b-46ff-8d9c-b50451380f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "su = fit.fit_transform(short_latents.cpu().detach().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f42de-a9dc-4e55-bf4d-481583144383",
   "metadata": {},
   "outputs": [],
   "source": [
    "su.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa1226-8892-450f-ae6f-081a75427d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=su[:,0], y=su[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5773c6a-0110-47b4-9c44-73c8bd4a191c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
