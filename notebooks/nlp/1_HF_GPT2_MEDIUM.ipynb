{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bf0544-0e78-4e1c-af3e-8130740193f9",
   "metadata": {},
   "source": [
    "# HUGGINGFACE TUTORIAL:\n",
    "https://huggingface.co/docs/transformers/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce054a-b4ff-4321-8b37-d6597c1de5ef",
   "metadata": {},
   "source": [
    "# PREREQUISITES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42cda4e-2825-4161-b78f-1773161ce86f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (4.26.0)\n",
      "Requirement already satisfied: datasets in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (2.9.0)\n",
      "Requirement already satisfied: evaluate in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: filelock in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: multiprocess in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: xxhash in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pandas in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: dill<0.3.7 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e760625c-d42f-4241-b2da-e632a0384ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ipywidgets>=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446934b-d213-4d55-b97c-eab04e576bc4",
   "metadata": {},
   "source": [
    "# DATASET & MODEL\n",
    "Start by loading a smaller subset of the r/askscience subset of the ELI5 dataset from the ðŸ¤— Datasets library. Thisâ€™ll give you a chance to experiment and make sure everythings works before spending more time training on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2603d52a-bf81-4103-b5c5-dbd63097f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461e7122-e882-456c-ab38-c4e1f45305eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (/home/dkarpeyev/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"eli5\"\n",
    "MODEL_NAME = \"gpt2-medium\"\n",
    "DATASET_SEGMENT_SIZE = 5000\n",
    "DATASET_SEGMENT_SIZE = -1\n",
    "dataset = load_dataset(DATASET_NAME, split=f\"train_asks[:{DATASET_SEGMENT_SIZE}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e41a1a-4dbe-4c88-a1fe-10b427813725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "    num_rows: 131777\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92dfab-95f9-4961-bc01-3a4860e170fe",
   "metadata": {},
   "source": [
    "Split the datasetâ€™s train_asks split into a train and test set with the train_test_split method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2efc68ba-5e1f-4c69-97f9-ff97a9673873",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_test = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d7292b-3ce4-466c-8f27-8b0be46df620",
   "metadata": {},
   "source": [
    "Then take a look at an example (we only need the 'text' field):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61072355-de9d-4e29-a995-dcd2f3d02a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': 'o5w78',\n",
       " 'title': 'Is the rate at which scientific discoveries are being made decreasing?',\n",
       " 'selftext': \"Seems like it's becoming harder for scientists to actually pin down discoveries (Higgs boson, dark matter and the like)\",\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers': {'a_id': ['c3en07a', 'c3emkb5'],\n",
       "  'text': [\"Many people, me among them, think that the rate of progress is in fact increasing, known as the Law of Accelerating Returns.  You can find a good explanation of it [here](_URL_2_).  If you look at cost effectiveness of [CPUs](_URL_1_), [hard drives](_URL_0_), [genomic reading](_URL_3_), and many, many, other important technologies (notice these graphs are basically straight lines on a log graph, which indicates an exponential trend) what you find is an exponential increase in performance per dollar over time (Moores Law is one example of this, for the number of transistors you can fit in an area for a price).\\n\\nAlthough not really scientific, someone went around asking experts in a bunch of different fields what they thought were the most important discoveries of all time and put them all on a timeline.  This timeline showed an exponential increase in the number of these discoveries over time, with very few spread out over long periods thousands of years ago until the recent explosion of great discoveries in the recent past.\\n\\nJust consider that from the invention of language, the wheel, farming, and tools happened over many thousands or tens of thousands of years, while the invention of the plane, the computer, genetics, cell phones, the internet, space flight, satellites, pacemakers, cochlear implants, and so much more has happened in just the last hundred.\\n\\nIf you also consider the number of people with access to the (near) sum of human knowledge, the ability to read and write, and the ability to publish information in a way that will never be lost to time is growing exponentially, it seems to indicate that the amount of scientific work that can and will be done must also be growing exponentially.\\n\\nSo given all of this, why does it seem to some people that there is not as much going on?  I think it is because the frontier of scientific knowledge is so far from an average education, that the really important breakthroughs are not obviously important to everyone since they cannot immediately grasp the concepts.  It is no longer possible for a single person, even an expert, to know half of what there is to know in a field today, so what does this mean for the layman?  Even if you were to only read the headlines of the new breakthroughs in modern science every day, i'm not sure you could read them much faster than they are coming in.  \\n\\nSo to understand WHY it is so important that we can now generate power from implanting bio fuel cells into bugs, or WHY it is important that a CAD like piece of software is being developed to allow people to design genetic structures on their computers you have to have a certain level of education about those topics, a level that most people just don't have.\",\n",
       "   \"What do you define as a discovery?\\n\\nThere are thousands of research papers submitted weekly.\\n\\nThere might be fewer fantasized headlines about science, but that's because magazines and TV stations don't report on most of the scientific discoveries that are made.\"],\n",
       "  'score': [17, 5]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': []},\n",
       " 'answers_urls': {'url': ['http://1.bp.blogspot.com/-LHoWVgcz0AM/TfJZaS5DZiI/AAAAAAAAPYw/HNZB_Rw-1L4/s1600/hd-cost-graph-small.jpg',\n",
       "   'http://www.singularity.com/charts/page61.html',\n",
       "   'http://www.kurzweilai.net/the-law-of-accelerating-returns',\n",
       "   'http://www.genome.gov/sequencingcosts/']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_test['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1754b0a-88a1-4785-82a8-631f160d576f",
   "metadata": {},
   "source": [
    "The next step is to load a DistilGPT2 tokenizer to process the text subfield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679728bc-550c-447b-825c-3e32745adfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57f4da5-a5df-41ce-b5cd-59b6df9bdfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2-medium', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd367c-f548-4a2d-9828-5bc0626609e3",
   "metadata": {},
   "source": [
    "Youâ€™ll notice from the example above, the text field is actually nested inside answers. This means youâ€™ll need to extract the text subfield from its nested structure with the flatten method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dad1bf77-dc2d-4336-8300-a61a70c4c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_test_flattened = dataset_train_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d7498b-1fb6-4a4b-acdd-8311422c3426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': 'o5w78',\n",
       " 'title': 'Is the rate at which scientific discoveries are being made decreasing?',\n",
       " 'selftext': \"Seems like it's becoming harder for scientists to actually pin down discoveries (Higgs boson, dark matter and the like)\",\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['c3en07a', 'c3emkb5'],\n",
       " 'answers.text': [\"Many people, me among them, think that the rate of progress is in fact increasing, known as the Law of Accelerating Returns.  You can find a good explanation of it [here](_URL_2_).  If you look at cost effectiveness of [CPUs](_URL_1_), [hard drives](_URL_0_), [genomic reading](_URL_3_), and many, many, other important technologies (notice these graphs are basically straight lines on a log graph, which indicates an exponential trend) what you find is an exponential increase in performance per dollar over time (Moores Law is one example of this, for the number of transistors you can fit in an area for a price).\\n\\nAlthough not really scientific, someone went around asking experts in a bunch of different fields what they thought were the most important discoveries of all time and put them all on a timeline.  This timeline showed an exponential increase in the number of these discoveries over time, with very few spread out over long periods thousands of years ago until the recent explosion of great discoveries in the recent past.\\n\\nJust consider that from the invention of language, the wheel, farming, and tools happened over many thousands or tens of thousands of years, while the invention of the plane, the computer, genetics, cell phones, the internet, space flight, satellites, pacemakers, cochlear implants, and so much more has happened in just the last hundred.\\n\\nIf you also consider the number of people with access to the (near) sum of human knowledge, the ability to read and write, and the ability to publish information in a way that will never be lost to time is growing exponentially, it seems to indicate that the amount of scientific work that can and will be done must also be growing exponentially.\\n\\nSo given all of this, why does it seem to some people that there is not as much going on?  I think it is because the frontier of scientific knowledge is so far from an average education, that the really important breakthroughs are not obviously important to everyone since they cannot immediately grasp the concepts.  It is no longer possible for a single person, even an expert, to know half of what there is to know in a field today, so what does this mean for the layman?  Even if you were to only read the headlines of the new breakthroughs in modern science every day, i'm not sure you could read them much faster than they are coming in.  \\n\\nSo to understand WHY it is so important that we can now generate power from implanting bio fuel cells into bugs, or WHY it is important that a CAD like piece of software is being developed to allow people to design genetic structures on their computers you have to have a certain level of education about those topics, a level that most people just don't have.\",\n",
       "  \"What do you define as a discovery?\\n\\nThere are thousands of research papers submitted weekly.\\n\\nThere might be fewer fantasized headlines about science, but that's because magazines and TV stations don't report on most of the scientific discoveries that are made.\"],\n",
       " 'answers.score': [17, 5],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': [],\n",
       " 'answers_urls.url': ['http://1.bp.blogspot.com/-LHoWVgcz0AM/TfJZaS5DZiI/AAAAAAAAPYw/HNZB_Rw-1L4/s1600/hd-cost-graph-small.jpg',\n",
       "  'http://www.singularity.com/charts/page61.html',\n",
       "  'http://www.kurzweilai.net/the-law-of-accelerating-returns',\n",
       "  'http://www.genome.gov/sequencingcosts/']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_test_flattened['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83fb8d-b3a7-451f-97e9-f2b17188fc79",
   "metadata": {},
   "source": [
    "Each subfield is now a separate column as indicated by the answers prefix, and the text field is a list now. Instead of tokenizing each sentence separately, convert the list to a string so you can jointly tokenize them.\n",
    "\n",
    "Here is how you can create a preprocessing function to convert the list to a string, and truncate sequences to be no longer than DistilGPT2â€™s maximum input length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5992675b-68a5-4ad5-bdb6-277c3ac6cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c83463-409e-4009-86c5-d7a62e613e56",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets with_transform method. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once, and increasing the number of processes with num_proc. Remove any columns you donâ€™t need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0aae341-3ba5-4c02-8e0a-9c286df1ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02187943458557129,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#0",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada55cb5b6974741bfd7f4d33e497f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0222320556640625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#1",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69e10dc186346ae8ee62e2ea7758f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021889209747314453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#2",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a9b5e1fd9342cc917c6f55d766b22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021190166473388672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#3",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e42c1794ee2461c99f5f5adc77e6687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020874738693237305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#0",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6056b462b1a84158bd06fbf9bc0afada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02114415168762207,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#1",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c1a29f752d48dbaecf6b2216050a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020645856857299805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#3",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449689a0de5749ea8bb5f9177ee0a225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021193265914916992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#2",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c75dd6d2c94820a6ecff4a9316f4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized = dataset_train_test_flattened.map(\n",
    "    preprocessor,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset_train_test_flattened[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fcb29e-4c91-40c3-ba19-d44af87512e7",
   "metadata": {},
   "source": [
    "Now youâ€™ll need a second preprocessing function to capture text truncated from the lengthier examples to avoid losing any information. This preprocessing function should:\n",
    "\n",
    "Concatenate all the text.\n",
    "Split the concatenated text into smaller chunks defined by block_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d300bc-e10f-4b13-83ec-81dcc49d2fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb51ff-5e09-469f-ab66-ed4e9f31fefe",
   "metadata": {},
   "source": [
    "Apply the group_texts function over the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f7b7fe5-1ae7-4b69-967f-1509961b3217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020387887954711914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#1",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67663d4fd2da469aac6121f06610b927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021906375885009766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#0",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b03e671a7b42ffb3e7d26e52cc9070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021754026412963867,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#3",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c457c2a565714d35bb094673534a5388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02252507209777832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#2",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2202cb7309d46f2bd6d68e2e4c58346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020123958587646484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#0",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82b382ec4f947529dff4194583de6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020008325576782227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#1",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7962e07c6d9c45e9881a29a05551c844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020168542861938477,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#2",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0557c482fc02456893bf839151238c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021090030670166016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#3",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483adedc5f5448cab5cae0d679778ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized_batched = dataset_tokenized.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66f0ad77-a05f-4846-ab10-c9eafce6f43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 267562\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 66573\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized_batched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d8e49-c451-44e0-b031-bad96b6d594c",
   "metadata": {},
   "source": [
    "Now create a batch of examples using DataCollatorForLanguageModeling. Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximium length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b0f97-88f6-4b27-9a35-e4b8a3ba3094",
   "metadata": {},
   "source": [
    "For causal language modeling, use the end-of-sequence token as the padding token and set mlm=False. This will use the inputs as labels shifted to the right by one element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b81289f5-afe6-415b-975f-debf9f98cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcbdda36-c80f-4ede-befd-62a2efb8a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1c62d-779f-484c-92ed-14a684932f8b",
   "metadata": {},
   "source": [
    "# Causal language modeling\n",
    "\n",
    "\n",
    "Causal language models are frequently used for text generation. This section shows you how to finetune DistilGPT2 to generate new text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57328d19-e82a-49c0-b9b1-3acd47a57a7c",
   "metadata": {},
   "source": [
    "If you arenâ€™t familiar with finetuning a model with the Trainer, take a look at the basic tutorial here!\n",
    "\n",
    "You're ready to start training your model now! Load DistilGPT2 with [AutoModelForCausalLM](/docs/transformers/v4.26.0/en/model_doc/auto#transformers.AutoModelForCausalLM):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4f2575c-7a65-4a1b-88a3-2268dd491304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189950f-5cc2-47d2-96f3-be6cc885e429",
   "metadata": {},
   "source": [
    "## DEBUG: BEGIN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91aaea2c-31e9-4ffe-8b04-723fca2ec773",
   "metadata": {},
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0dcb95a-bfae-450c-9647-86b79c76fd70",
   "metadata": {},
   "source": [
    "isinstance(model, nn.Module)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8457f1ce-dabf-40a7-bd20-6c71d3d0a974",
   "metadata": {},
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39aea920-529c-4945-8a94-d4be888f7684",
   "metadata": {},
   "source": [
    "type(model.forward(torch.tensor(dataset_tokenized_batched['test']['input_ids'][0])))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0302f08e-0d31-46be-aecd-6f723e46df5f",
   "metadata": {},
   "source": [
    "torch.tensor(dataset_tokenized_batched['test']['input_ids'][0]).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "324deb4b-45da-4a67-9735-05f3e8fa85ea",
   "metadata": {},
   "source": [
    "model.forward(torch.tensor(dataset_tokenized_batched['test']['input_ids'][0])).logits.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "107cf992-cb0e-430b-82d9-07eb48d6bdf9",
   "metadata": {},
   "source": [
    "input = torch.tensor(dataset_tokenized_batched['test']['input_ids'][0])\n",
    "input"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40319b09-64ff-4a94-8a48-6eae0e6b0559",
   "metadata": {},
   "source": [
    "len(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad54fd5c-75db-49d2-b167-be3a991f127c",
   "metadata": {},
   "source": [
    "labels = torch.tensor(dataset_tokenized_batched['test']['labels'][0])\n",
    "labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d6f674d-bc12-4e29-8d55-f03671676ae3",
   "metadata": {},
   "source": [
    "tokenizer.decode(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c029e9f-ee2e-4568-bf37-37a30ff62899",
   "metadata": {
    "tags": []
   },
   "source": [
    "output = model(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81d35ba5-4b62-416c-9da4-1a2c32e3e972",
   "metadata": {},
   "source": [
    "out = output.logits.softmax(1).max(1).indices\n",
    "out"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9632faa-d93c-4880-a470-7e8208812bde",
   "metadata": {},
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7425a3bd-07ef-46b3-8d08-78aae36748a8",
   "metadata": {},
   "source": [
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "062fd15d-3a4c-4632-91c0-bf69eba02875",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2943b7c1-fa71-4f82-8ca1-6c72adf871cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46226bb6-076f-405d-be32-ac95acec3437",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a83c079-3983-4743-a20c-e7670b492805",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b29b88f5-ca7c-41b1-aa40-1a39f3f599fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my name\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98deed86-d2cd-4575-9171-66e91fb4089b",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my name is\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04ef0913-467f-4c87-a852-2899109f7dff",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my name is Bob\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc2a9138-0a82-4e47-8b9d-7fe5af5f9c63",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my name is Bob. What\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f2ea876-31fa-4833-9404-2eb1d7decf9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my name is Bob. What is\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1663f1b-4a85-427e-9074-9ca2c6b7887c",
   "metadata": {},
   "source": [
    "tokenizer(\"Hello, my name is Bob. What is\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fa7a71e-f999-466b-829b-a70eb016311f",
   "metadata": {
    "tags": []
   },
   "source": [
    "output = model(input_ids=tokenizer(\"Hello, my name is Bob. What is\", return_tensors=\"pt\")['input_ids'], \n",
    "                       labels=tokenizer(\", my name is Bob. What is your\", return_tensors=\"pt\")['input_ids'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "842384ab-c4b4-4a20-8894-571e2f1e1e17",
   "metadata": {},
   "source": [
    "tokenizer.decode(output.logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d340ca8-2ac0-402f-bacc-40a13c108c61",
   "metadata": {
    "tags": []
   },
   "source": [
    "output.loss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c6bc580-225a-4c29-ae2f-749b95667aaf",
   "metadata": {},
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33bd630-803f-4f12-bc87-cddf66c7d2e7",
   "metadata": {},
   "source": [
    "## DEBUG: END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc3d07-780f-41a9-95c1-f44a06a1c8d4",
   "metadata": {},
   "source": [
    "### Training\n",
    "## [Fails to run on specificed GPU -- defaults to \"cuda:0\" -- due to a bug in `transformers`]\n",
    "At this point, only three steps remain:\n",
    "\n",
    "Define your training hyperparameters in TrainingArguments. The only required parameter is output_dir which specifies where to save your model. Youâ€™ll push this model to the Hub by setting push_to_hub=True (you need to be signed in to Hugging Face to upload your model).\n",
    "Pass the training arguments to Trainer along with the model, datasets, and data collator.\n",
    "Call train() to finetune your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26708f55-eb45-40d0-bf46-63d6822c61f6",
   "metadata": {},
   "source": [
    "### Disable external telemetry (this may not be reachable from the local network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31adc1d5-d287-4950-9a63-09598f5a235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf85403e-ac95-487a-b721-e17cd937078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901ab61b-c5dd-4096-b3fb-b69c1f74f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CUDA environment BEFORE importing torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{GPU}\"  # This shrinks the GPU universe and maps cuda:0 to {GPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddc80903-c9bb-4753-8e9a-04748ef058ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98364ad5-5c84-4c6c-9b2a-a44e5cde7848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device() # This really is device {GPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d516a66-8dd6-41ea-a4c2-c522a1fbf81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1fbe7cb-550c-480d-8ed7-30b210c6ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR=None\n",
    "if CHECKPOINT_DIR is not None:\n",
    "    model = model.from_pretrained(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dec0a2e4-8319-4f5a-9037-a5de216146ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, I'm a language. I'm a compiler, I'm a parser, I'm a server process. I\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'd like to join an existing team. What can I do to get started?\\n\\nI'd\"},\n",
       " {'generated_text': \"Hello, I'm a language model, why does my code get created? Can't I just copy it? But why did my code get created when\"},\n",
       " {'generated_text': \"Hello, I'm a language model, a functional language...\\n\\nI'm a functional language. Is it hard? A little, yes. But\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I need to give me objects from which I can get\"}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77ff6093-944f-4ec1-8242-c9de25999d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: I highly doubt there to be scientific experimentation about this.  \n",
      "The answer is relative and would depend on how much the person sweats during the day, and at what times. This isn't a scientific question. If your bed is dirty and you aren't changing the sheets, then morning shower is better. But I can't go to bed with the day's disgustingness all over me so I usually shower before bed. If I sat around home all day doing nothing, I might skip the night shower and do it in the morning.Temperatures for stars are easily determined - different elements produce very specific light signatures, and the signature\n",
      "inputs_0[:10]: I highly d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I highly dificulty: my husband has an allergy to peanuts and we think they help relieve the discomfort of the allergic joint. It is also the'},\n",
       " {'generated_text': 'I highly dicuss, and will send an email when a possible new topic is chosen.'},\n",
       " {'generated_text': 'I highly dived down to listen for that, and for how this could take a few more years with more work on that front, all the way'},\n",
       " {'generated_text': 'I highly dicuss the idea of sending you an item in the mail with the contents in plain text so you can look it over before sending it'},\n",
       " {'generated_text': 'I highly dnactified my father\\'s letter! It seemed so true.\" It was really not a letter, it was the vision of a person'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 10\n",
    "input_ids_0 = torch.tensor(dataset_tokenized_batched[\"test\"][\"input_ids\"][0])\n",
    "inputs_0 = tokenizer.decode(input_ids_0)\n",
    "label_ids_0 = torch.tensor(dataset_tokenized_batched[\"test\"][\"labels\"][0])\n",
    "labels_0 = tokenizer.decode(label_ids_0)\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:{PRIME_LEN}]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6858aa5f-8804-46b6-a2ba-6ac0c0c315c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: I have to tell ya honestly\n",
      "inputs_0[:30]: I have to tell ya honestly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I have to tell ya honestly, I\\'ve had it with everyone who\\'s running and talking about the Olympics.\"\\n\\nWagner\\'s father is'},\n",
       " {'generated_text': 'I have to tell ya honestly, I\\'d rather not play football. We\\'ll watch the game again. This time it could have been worse.\"\\n'},\n",
       " {'generated_text': 'I have to tell ya honestly,\" said the detective, \"but there\\'s an interesting story behind that one. I\\'m not sure I even got a'},\n",
       " {'generated_text': 'I have to tell ya honestly I\\'m starting to like these things.\"'},\n",
       " {'generated_text': \"I have to tell ya honestly, he's kind of fun! He's funny. I see him coming out of a booth all the time, asking\"}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 30\n",
    "inputs_0 = \"I have to tell ya honestly\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:{PRIME_LEN}]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2223253-b63e-4ee6-af82-14b691f56cca",
   "metadata": {},
   "source": [
    "TrainingArguments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abe2c2aa-fcc9-4107-b216-10c54613ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "time = datetime.datetime.now().strftime('%H.%M')\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{MODEL_NAME}-{DATASET_NAME}/date={date}/time={time}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    #place_model_on_device=torch.device(f\"cuda:{GPU}\"),\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=3.0,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized_batched[\"train\"],\n",
    "    eval_dataset=dataset_tokenized_batched[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3115025c-743c-4cdf-bda6-0b004fa6bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 267562\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100338\n",
      "  Number of trainable parameters = 354823168\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100338' max='100338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100338/100338 3:15:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.143800</td>\n",
       "      <td>3.079568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.053500</td>\n",
       "      <td>3.053271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.002800</td>\n",
       "      <td>3.046569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-1000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-1000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-1000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-1500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-1500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-1500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-2000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-2000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-2000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-2500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-2500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-2500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-3000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-3000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-3000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-3500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-3500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-3500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-4000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-4000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-4000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-4500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-4500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-4500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-5000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-5000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-5000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-5500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-5500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-5500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-6000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-6000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-6000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-6500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-6500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-6500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-7000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-7000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-7000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-7500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-7500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-7500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-8000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-8000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-8000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-8500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-8500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-8500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-9000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-9000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-9000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-9500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-9500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-9500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-10000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-10000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-10000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-10500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-10500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-10500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-11000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-11000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-11000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-11500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-11500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-11500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-12000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-12000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-12000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-12500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-12500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-12500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-13000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-13000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-13000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-13500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-13500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-13500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-14000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-14000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-14000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-14500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-14500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-14500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-15000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-15000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-15000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-15500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-15500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-15500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-16000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-16000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-16000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-16500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-16500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-16500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-16500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-17000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-17000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-17000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-17500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-17500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-17500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-18000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-18000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-18000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-18500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-18500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-18500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-18500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-19000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-19000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-19000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-19000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-19500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-19500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-19500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-19500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-20000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-20000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-20000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-20500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-20500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-20500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-20500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-21000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-21000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-21000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-21500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-21500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-21500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-21500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-22000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-22000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-22000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-22000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-22500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-22500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-22500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-22500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-23000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-23000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-23000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-23000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-23500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-23500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-23500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-23500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-24000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-24000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-24000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-24000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-24500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-24500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-24500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-24500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-25000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-25000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-25000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-25000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-25500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-25500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-25500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-25500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-26000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-26000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-26000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-26000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-26500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-26500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-26500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-26500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-27000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-27000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-27000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-27000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-27500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-27500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-27500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-27500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-28000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-28000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-28000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-28000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-28500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-28500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-28500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-28500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-29000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-29000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-29000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-29000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-29500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-29500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-29500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-29500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-30000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-30000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-30000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-30000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-30500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-30500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-30500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-30500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-31000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-31000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-31000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-31000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-31500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-31500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-31500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-31500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-32000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-32000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-32000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-32000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-32500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-32500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-32500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-32500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-33000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-33000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-33000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-33000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66573\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-33500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-33500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-33500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-33500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-34000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-34000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-34000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-34000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-34500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-34500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-34500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-34500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-35000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-35000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-35000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-35000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-35500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-35500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-35500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-35500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-36000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-36000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-36000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-36000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-36500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-36500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-36500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-36500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-37000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-37000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-37000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-37000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-37500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-37500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-37500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-37500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-38000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-38000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-38000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-38000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-38500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-38500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-38500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-38500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-39000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-39000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-39000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-39000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-39500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-39500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-39500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-39500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-40000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-40000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-40000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-40000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-40500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-40500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-40500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-40500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-41000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-41000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-41000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-41000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-41500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-41500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-41500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-41500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-42000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-42000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-42000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-42000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-42500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-42500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-42500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-42500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-43000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-43000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-43000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-43000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-43500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-43500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-43500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-43500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-44000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-44000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-44000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-44000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-44500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-44500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-44500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-44500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-45000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-45000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-45000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-45000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-45500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-45500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-45500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-45500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-46000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-46000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-46000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-46000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-46500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-46500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-46500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-46500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-47000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-47000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-47000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-47000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-47500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-47500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-47500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-47500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-48000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-48000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-48000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-48000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-48500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-48500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-48500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-48500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-49000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-49000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-49000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-49000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-49500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-49500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-49500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-49500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-50000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-50000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-50000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-50000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-50500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-50500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-50500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-50500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-51000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-51000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-51000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-51000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-51500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-51500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-51500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-51500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-52000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-52000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-52000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-52000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-52500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-52500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-52500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-52500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-53000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-53000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-53000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-53000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-53500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-53500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-53500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-53500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-54000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-54000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-54000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-54000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-54500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-54500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-54500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-54500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-55000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-55000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-55000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-55000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-55500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-55500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-55500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-55500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-56000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-56000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-56000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-56000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-56500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-56500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-56500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-56500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-57000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-57000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-57000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-57000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-57500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-57500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-57500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-57500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-58000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-58000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-58000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-58000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-58500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-58500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-58500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-58500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-59000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-59000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-59000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-59000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-59500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-59500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-59500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-59500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-60000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-60000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-60000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-60000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-60500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-60500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-60500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-60500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-61000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-61000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-61000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-61000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-61500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-61500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-61500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-61500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-62500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-62500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-62500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-62500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-63000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-63000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-63000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-63000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-63500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-63500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-63500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-63500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-64000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-64000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-64000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-64000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-64500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-64500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-64500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-64500/pytorch_model.bin\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-75500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-75500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-75500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-75500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-76000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-76000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-76000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-76000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-76500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-76500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-76500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-76500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-77000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-77000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-77000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-77000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-77500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-77500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-77500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-77500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-78000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-78000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-78000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-78000/pytorch_model.bin\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-81000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-81000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-81000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-81000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-81500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-81500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-81500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-81500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-82000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-82000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-82000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-82000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-82500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-82500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-82500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-82500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-83000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-83000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-83000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-83000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-83500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-83500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-83500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-83500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-84000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-84000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-84000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-84000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-84500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-84500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-84500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-84500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-85000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-85000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-85000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-85000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-85500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-85500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-85500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-85500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-86000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-86000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-86000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-86000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-86500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-86500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-86500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-86500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-87000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-87000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-87000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-87000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-87500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-87500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-87500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-87500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-88000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-88000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-88000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-88000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-88500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-88500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-88500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-88500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-89000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-89000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-89000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-89000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-89500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-89500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-89500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-89500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-90000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-90000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-90000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-90000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-90500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-90500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-90500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-90500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-91000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-91000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-91000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-91000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-91500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-91500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-91500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-91500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-92000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-92000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-92000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-92000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-92500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-92500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-92500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-92500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-93000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-93000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-93000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-93000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-93500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-93500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-93500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-93500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-94000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-94000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-94000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-94000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-94500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-94500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-94500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-94500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-95000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-95000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-95000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-95000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-95500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-95500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-95500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-95500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-96000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-96000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-96000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-96000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-96500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-96500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-96500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-96500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-97000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-97000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-97000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-97000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-97500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-97500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-97500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-97500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-98000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-98000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-98000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-98000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-98500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-98500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-98500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-98500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-99000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-99000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-99000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-99000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-99500\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-99500/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-99500/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-99500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-100000\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-100000/config.json\n",
      "Configuration saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-100000/generation_config.json\n",
      "Model weights saved in gpt2-medium-eli5/date=2023-02-10/time=06.25/checkpoint-100000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66573\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100338, training_loss=3.0815638375445293, metrics={'train_runtime': 11711.3688, 'train_samples_per_second': 68.539, 'train_steps_per_second': 8.568, 'total_flos': 1.8636376142197555e+17, 'train_loss': 3.0815638375445293, 'epoch': 3.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "460f9f85-54d6-4eae-a352-18940ab456d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: I highly doubt there to be scientific experimentation about this.  \n",
      "The answer is relative and would depend on how much the person sweats during the day, and at what times. This isn't a scientific question. If your bed is dirty and you aren't changing the sheets, then morning shower is better. But I can't go to bed with the day's disgustingness all over me so I usually shower before bed. If I sat around home all day doing nothing, I might skip the night shower and do it in the morning.Temperatures for stars are easily determined - different elements produce very specific light signatures, and the signature\n",
      "inputs_0[:10]: I highly d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I highly dificulty in my research. In my limited time working on this thing, I have not come anywhere close to doing what you are asking'},\n",
       " {'generated_text': \"I highly dicuss, and will continue doing so, a very broad topic regarding science. This is a fun, interesting question. I'll leave\"},\n",
       " {'generated_text': \"I highly dificult to add that I think there's an interesting idea in there somewhere. I'm not sure on how it is resolved, but\"},\n",
       " {'generated_text': 'I highly dicuss the idea of artificial gravity and what that would effect the universe. If no significant effects from gravity is being caused i dont see'},\n",
       " {'generated_text': 'I highly dabbled in my studies of the subject as well. I think the general consensus is that the short answer is yes. One of the'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "PRIME_LEN = 10\n",
    "input_ids_0 = torch.tensor(dataset_tokenized_batched[\"test\"][\"input_ids\"][0])\n",
    "inputs_0 = tokenizer.decode(input_ids_0)\n",
    "label_ids_0 = torch.tensor(dataset_tokenized_batched[\"test\"][\"labels\"][0])\n",
    "labels_0 = tokenizer.decode(label_ids_0)\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:{PRIME_LEN}]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "model.to(\"cpu\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd10a728-bdd4-4b3a-b8db-4b6753eb7431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: I have to tell ya honestly\n",
      "inputs_0[:PRIME_LEN]: I have to tell ya honestly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I have to tell ya honestly I've never once thought to take a long time after I'm done using an electric razor.  Especially when it's\"},\n",
       " {'generated_text': \"I have to tell ya honestly, I'd be happy to tell you how a human can produce a photon.  So I have to do a bit\"},\n",
       " {'generated_text': 'I have to tell ya honestly that this is a really cool question! As an undergrad, I studied chemical dynamics/material science and chemistry/physics'},\n",
       " {'generated_text': \"I have to tell ya honestly I'm not really a scientist in these matters, so I'm probably not a great judge of their accuracy, but this\"},\n",
       " {'generated_text': 'I have to tell ya honestly, I like my dogs because they provide a constant amount of comfort or attention.  It really is a good bonding experience'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 30\n",
    "inputs_0 = \"I have to tell ya honestly\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:PRIME_LEN]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "model.to(\"cpu\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0061cb9a-1261-448f-9c8a-701436f39c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: I have to tell ya hone\n",
      "inputs_0[:PRIME_LEN]: I have to tell ya hone\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I have to tell ya honeysuckle gets a bad rap for not having a lot of flavor; I've heard it's not really nearly as sweet\"},\n",
       " {'generated_text': 'I have to tell ya honeysuckle is one of those things that people say makes you ugly - I have read it is associated with cancer (just'},\n",
       " {'generated_text': \"I have to tell ya honeysuckle is a really cool plant! Here's one to take a look at.  \\n_URL_0\"},\n",
       " {'generated_text': \"I have to tell ya honeysuckle is really hard to keep alive. It's incredibly toxic and hard to chew, so you'd have to keep\"},\n",
       " {'generated_text': 'I have to tell ya honeysuckle is my absolute favorite! It smells like burning wood, or burning your eyes from the inside out, or burning'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 30\n",
    "inputs_0 = \"I have to tell ya hone\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:PRIME_LEN]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "model.to(\"cpu\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2747eefb-781a-48cf-9da2-8c7acce489dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: Please explain AI to me.\n",
      "inputs_0[:3000]: Please explain AI to me.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Please explain AI to me. I've never once thought to think about it, and I think the universe seems to be thinking the same way. It\"},\n",
       " {'generated_text': \"Please explain AI to me. What exactly is it/would it do? How would it be able to handle complex systems efficiently?I'll assume there\"},\n",
       " {'generated_text': 'Please explain AI to me. Can you give examples for how its made? What can you tell me about the problem? \\n\\nI am curious'},\n",
       " {'generated_text': \"Please explain AI to me. I'm not really equipped to answer these questions, and I'm probably not a great person to be answering questions about this\"},\n",
       " {'generated_text': \"Please explain AI to me.  If you can't, I'll assume you are talking in terms of computer programs.  I've never seen an\"}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 3000\n",
    "inputs_0 = \"Please explain AI to me.\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:{PRIME_LEN}]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c26a2bc3-c114-41d0-a0c3-e798b088fefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: Please explain AI to me:\n",
      "inputs_0[:3000]: Please explain AI to me:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Please explain AI to me:) I would imagine it would take some time, and we aren't sure yet if it's faster than humans. It\"},\n",
       " {'generated_text': 'Please explain AI to me: What is it and how does it work? How is it related to science? What does it have to do with science'},\n",
       " {'generated_text': 'Please explain AI to me: why does the sky get blue, and why is this a good idea? > I thought that the world is composed of'},\n",
       " {'generated_text': 'Please explain AI to me: a computer can do a lot of things that humans have never been able to in a conscious effort.  AI, as'},\n",
       " {'generated_text': 'Please explain AI to me: is there something like \"human\" intelligence as we understand it in our definition? Like a human is a machine, or'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 3000\n",
    "inputs_0 = \"Please explain AI to me:\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:{PRIME_LEN}]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bcb09ce9-e687-4929-b7c7-f3863d7adb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: Please explain AI to me.\n",
      "inputs_0[:3000]: Please explain AI to me.\n",
      "CPU times: user 3h 3s, sys: 2min 12s, total: 3h 2min 16s\n",
      "Wall time: 9min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Please explain AI to me. I\\'m currently a 3 year old girl.  Is it possible that one day she would be able to understand me?\\n\\nI mean come on, what\\'s the big deal. Can\\'t she see how silly I am, then try to teach me how to walk? How can a computer (if I take the word literally) understand someone or help me play? All I know is that we are constantly doing something we\\'re not supposed to do?       \\n\\nI can\\'t think of a single reason why an AI couldn\\'t be possible. I mean, look at her and her tiny little head twitch and blink and her hands move?  I\\'d love to hear her thoughts, but isn\\'t there some sort of \\'program\\' that\\'s actually running inside her brain to help her?  \\n\\nSo basically, what do you think of when you think of an AI?  How would it know how to get in the way of the \\'programs\\' the user is creating?  Imagine a creature that could not think but still could control her physical body and walk around?        \\n\\nAnd also could not reproduce--no problem, just more efficient.        \\n\\nI\\'m going to have to be very careful what I say here because I\\'m sure the other commenters will start out saying \"but wait, that just puts all human thought into some'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "PRIME_LEN = 3000\n",
    "inputs_0 = \"Please explain AI to me.\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:{PRIME_LEN}]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "model.to(torch.device(\"cpu\"))\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=300, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47b48c96-a508-4b69-84b6-41ee717816fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: Russia has invaded\n",
      "inputs_0[:3000]: Russia has invaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Russia has invaded Russia and Russia has been invaded. The question is, what are the legal consequences of that? Russia doesn't have a law which deals\"},\n",
       " {'generated_text': \"Russia has invaded Syria in the 1970's. They have a nuclear arsenal as well.\\n\\nSo, yes - you might want to keep your nuclear\"},\n",
       " {'generated_text': \"Russia has invaded the US?**\\n\\n**From my quick research**\\n\\nIt's hard to say, but given the very low cost to\"},\n",
       " {'generated_text': 'Russia has invaded and occupied the part of Georgia it has controlled for several years. This is a legitimate invasion by a major power, not an international crime'},\n",
       " {'generated_text': 'Russia has invaded, it\\'s the US that has invaded\". This is actually quite popular with the public because the phrase \"invasion\" is used to'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 3000\n",
    "inputs_0 = \"Russia has invaded\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:{PRIME_LEN}]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(41)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282c0f6-f00a-4f7f-841e-2a62cd00e9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
