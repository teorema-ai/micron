{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bf0544-0e78-4e1c-af3e-8130740193f9",
   "metadata": {},
   "source": [
    "# HUGGINGFACE TUTORIAL:\n",
    "https://huggingface.co/docs/transformers/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce054a-b4ff-4321-8b37-d6597c1de5ef",
   "metadata": {},
   "source": [
    "# PREREQUISITES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42cda4e-2825-4161-b78f-1773161ce86f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (4.26.0)\n",
      "Requirement already satisfied: datasets in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (2.9.0)\n",
      "Requirement already satisfied: evaluate in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: pandas in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: multiprocess in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: dill<0.3.7 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/data/home/dkarpeyev/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/data/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e760625c-d42f-4241-b2da-e632a0384ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ipywidgets>=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446934b-d213-4d55-b97c-eab04e576bc4",
   "metadata": {},
   "source": [
    "# DATASET & MODEL\n",
    "Start by loading a smaller subset of the r/askscience subset of the ELI5 dataset from the ðŸ¤— Datasets library. Thisâ€™ll give you a chance to experiment and make sure everythings works before spending more time training on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2603d52a-bf81-4103-b5c5-dbd63097f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461e7122-e882-456c-ab38-c4e1f45305eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (/home/dkarpeyev/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"eli5\"\n",
    "MODEL_NAME = \"gpt2\"\n",
    "DATASET_SEGMENT_SIZE = 5000\n",
    "DATASET_SEGMENT_SIZE = -1\n",
    "dataset = load_dataset(DATASET_NAME, split=f\"train_asks[:{DATASET_SEGMENT_SIZE}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e41a1a-4dbe-4c88-a1fe-10b427813725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "    num_rows: 131777\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92dfab-95f9-4961-bc01-3a4860e170fe",
   "metadata": {},
   "source": [
    "Split the datasetâ€™s train_asks split into a train and test set with the train_test_split method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2efc68ba-5e1f-4c69-97f9-ff97a9673873",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_test = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d7292b-3ce4-466c-8f27-8b0be46df620",
   "metadata": {},
   "source": [
    "Then take a look at an example (we only need the 'text' field):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61072355-de9d-4e29-a995-dcd2f3d02a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': 'koozm',\n",
       " 'title': 'Why do NSAIDS antagonize THC effects?',\n",
       " 'selftext': 'By personal experience, anecdotical evidence and [this article](_URL_0_) I know that Non steroidal anti inflammatory drugs like aspirin and iboprufen antagonize THC effects. In other words they sober you up very fast. Can anyone here explain me why they do it? \\n\\nThanks, guys.',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers': {'a_id': ['c2m9z55'],\n",
       "  'text': [\"I'm going to try and make this is as coherent as possible...but it's tricky.  THC does its thing by binding to a cannabinoid receptor (CB1), which is where most of the high comes from. \\n\\nThe reason why you have cannabinoid receptors in the first place is that you have an endogenous cannabinoid system, with anandamide being the main poster child of that.  Anandamide is derived from phospholipds (arachadonic acid).\\n\\nNSAIDs block the enzyme cyclooxygenase (COX), which normally makes prostaglandins that have roles in inflammation/fever/etc.  Prostaglandins are derived from arachadonic acid, similar to anandamide.  Therefore, the systems definitely share a common denominator and probably have lots of complicated interactions. \\n\\nYour article linkes to this: [study](_URL_0_). Apparently cannabinoids like THC cause induction of arachidonic acid and create prostaglandins and somehow they aid in the 'high'.  \\n\\nThis [article](_URL_1_) says that  chronic THC alters the cyclooxygenase system, decreasing the potency and efficacy of NSAIDs used.  I'm assuming this works in reverse, but it's most likely a very, very complicated interaction depending on how often you smoke.\"],\n",
       "  'score': [3]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': ['http://www.cannabis-med.org/english/faq/02-interaction.htm']},\n",
       " 'answers_urls': {'url': ['http://www.ncbi.nlm.nih.gov/pubmed/1846934',\n",
       "   'http://www.ncbi.nlm.nih.gov/pubmed/12235269']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_test['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1754b0a-88a1-4785-82a8-631f160d576f",
   "metadata": {},
   "source": [
    "The next step is to load a DistilGPT2 tokenizer to process the text subfield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679728bc-550c-447b-825c-3e32745adfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57f4da5-a5df-41ce-b5cd-59b6df9bdfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd367c-f548-4a2d-9828-5bc0626609e3",
   "metadata": {},
   "source": [
    "Youâ€™ll notice from the example above, the text field is actually nested inside answers. This means youâ€™ll need to extract the text subfield from its nested structure with the flatten method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dad1bf77-dc2d-4336-8300-a61a70c4c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_test_flattened = dataset_train_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d7498b-1fb6-4a4b-acdd-8311422c3426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': 'koozm',\n",
       " 'title': 'Why do NSAIDS antagonize THC effects?',\n",
       " 'selftext': 'By personal experience, anecdotical evidence and [this article](_URL_0_) I know that Non steroidal anti inflammatory drugs like aspirin and iboprufen antagonize THC effects. In other words they sober you up very fast. Can anyone here explain me why they do it? \\n\\nThanks, guys.',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['c2m9z55'],\n",
       " 'answers.text': [\"I'm going to try and make this is as coherent as possible...but it's tricky.  THC does its thing by binding to a cannabinoid receptor (CB1), which is where most of the high comes from. \\n\\nThe reason why you have cannabinoid receptors in the first place is that you have an endogenous cannabinoid system, with anandamide being the main poster child of that.  Anandamide is derived from phospholipds (arachadonic acid).\\n\\nNSAIDs block the enzyme cyclooxygenase (COX), which normally makes prostaglandins that have roles in inflammation/fever/etc.  Prostaglandins are derived from arachadonic acid, similar to anandamide.  Therefore, the systems definitely share a common denominator and probably have lots of complicated interactions. \\n\\nYour article linkes to this: [study](_URL_0_). Apparently cannabinoids like THC cause induction of arachidonic acid and create prostaglandins and somehow they aid in the 'high'.  \\n\\nThis [article](_URL_1_) says that  chronic THC alters the cyclooxygenase system, decreasing the potency and efficacy of NSAIDs used.  I'm assuming this works in reverse, but it's most likely a very, very complicated interaction depending on how often you smoke.\"],\n",
       " 'answers.score': [3],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': ['http://www.cannabis-med.org/english/faq/02-interaction.htm'],\n",
       " 'answers_urls.url': ['http://www.ncbi.nlm.nih.gov/pubmed/1846934',\n",
       "  'http://www.ncbi.nlm.nih.gov/pubmed/12235269']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_test_flattened['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83fb8d-b3a7-451f-97e9-f2b17188fc79",
   "metadata": {},
   "source": [
    "Each subfield is now a separate column as indicated by the answers prefix, and the text field is a list now. Instead of tokenizing each sentence separately, convert the list to a string so you can jointly tokenize them.\n",
    "\n",
    "Here is how you can create a preprocessing function to convert the list to a string, and truncate sequences to be no longer than DistilGPT2â€™s maximum input length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5992675b-68a5-4ad5-bdb6-277c3ac6cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c83463-409e-4009-86c5-d7a62e613e56",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets with_transform method. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once, and increasing the number of processes with num_proc. Remove any columns you donâ€™t need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0aae341-3ba5-4c02-8e0a-9c286df1ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02337360382080078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#0",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0821c62b904083b2d947621ee68e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02142953872680664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#1",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1952ab2f636e4c51b5db37c8676afbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020555734634399414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#2",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d5a1f937dd4e62b5e72e116aad682d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020898103713989258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#3",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f196516f5be649229f490d1b8306ac17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021526336669921875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#0",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e33769094b2462c9a9485e39e3e8299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020537614822387695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#3",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb34dd4bc914077975e57752f62c1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021509647369384766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#2",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09169e57c7449aaabab6fea5aaf6000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02120065689086914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#1",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7f612c718841d897d882ad8002337c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized = dataset_train_test_flattened.map(\n",
    "    preprocessor,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset_train_test_flattened[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fcb29e-4c91-40c3-ba19-d44af87512e7",
   "metadata": {},
   "source": [
    "Now youâ€™ll need a second preprocessing function to capture text truncated from the lengthier examples to avoid losing any information. This preprocessing function should:\n",
    "\n",
    "Concatenate all the text.\n",
    "Split the concatenated text into smaller chunks defined by block_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d300bc-e10f-4b13-83ec-81dcc49d2fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb51ff-5e09-469f-ab66-ed4e9f31fefe",
   "metadata": {},
   "source": [
    "Apply the group_texts function over the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f7b7fe5-1ae7-4b69-967f-1509961b3217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021025419235229492,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#0",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4392a21d6d8f490d939d53427766bac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020585060119628906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#1",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368c5e44629141ce8c21f01d3ea62b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02056097984313965,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#2",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012d7b86896d4909990c548b4b1aac5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02077484130859375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#3",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6603d9dbe344139710614becabc7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02097916603088379,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#0",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080276a87f3e4afeaa2752aca54efff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0209958553314209,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#2",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4681ac92e314c428dc3996c33a774da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02224898338317871,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#1",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65586bce47b145d486f4c44b4af1b22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021033287048339844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#3",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f7b8d4f73043348004b9fde97af67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized_batched = dataset_tokenized.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66f0ad77-a05f-4846-ab10-c9eafce6f43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 267621\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 66508\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized_batched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d8e49-c451-44e0-b031-bad96b6d594c",
   "metadata": {},
   "source": [
    "Now create a batch of examples using DataCollatorForLanguageModeling. Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximium length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b0f97-88f6-4b27-9a35-e4b8a3ba3094",
   "metadata": {},
   "source": [
    "For causal language modeling, use the end-of-sequence token as the padding token and set mlm=False. This will use the inputs as labels shifted to the right by one element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b81289f5-afe6-415b-975f-debf9f98cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcbdda36-c80f-4ede-befd-62a2efb8a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1c62d-779f-484c-92ed-14a684932f8b",
   "metadata": {},
   "source": [
    "# Causal language modeling\n",
    "\n",
    "\n",
    "Causal language models are frequently used for text generation. This section shows you how to finetune DistilGPT2 to generate new text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57328d19-e82a-49c0-b9b1-3acd47a57a7c",
   "metadata": {},
   "source": [
    "If you arenâ€™t familiar with finetuning a model with the Trainer, take a look at the basic tutorial here!\n",
    "\n",
    "You're ready to start training your model now! Load DistilGPT2 with [AutoModelForCausalLM](/docs/transformers/v4.26.0/en/model_doc/auto#transformers.AutoModelForCausalLM):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4f2575c-7a65-4a1b-88a3-2268dd491304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189950f-5cc2-47d2-96f3-be6cc885e429",
   "metadata": {},
   "source": [
    "## DEBUG: BEGIN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91aaea2c-31e9-4ffe-8b04-723fca2ec773",
   "metadata": {},
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0dcb95a-bfae-450c-9647-86b79c76fd70",
   "metadata": {},
   "source": [
    "isinstance(model, nn.Module)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8457f1ce-dabf-40a7-bd20-6c71d3d0a974",
   "metadata": {},
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39aea920-529c-4945-8a94-d4be888f7684",
   "metadata": {},
   "source": [
    "type(model.forward(torch.tensor(dataset_tokenized_batched['test']['input_ids'][0])))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0302f08e-0d31-46be-aecd-6f723e46df5f",
   "metadata": {},
   "source": [
    "torch.tensor(dataset_tokenized_batched['test']['input_ids'][0]).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "324deb4b-45da-4a67-9735-05f3e8fa85ea",
   "metadata": {},
   "source": [
    "model.forward(torch.tensor(dataset_tokenized_batched['test']['input_ids'][0])).logits.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "107cf992-cb0e-430b-82d9-07eb48d6bdf9",
   "metadata": {},
   "source": [
    "input = torch.tensor(dataset_tokenized_batched['test']['input_ids'][0])\n",
    "input"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40319b09-64ff-4a94-8a48-6eae0e6b0559",
   "metadata": {},
   "source": [
    "len(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad54fd5c-75db-49d2-b167-be3a991f127c",
   "metadata": {},
   "source": [
    "labels = torch.tensor(dataset_tokenized_batched['test']['labels'][0])\n",
    "labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d6f674d-bc12-4e29-8d55-f03671676ae3",
   "metadata": {},
   "source": [
    "tokenizer.decode(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c029e9f-ee2e-4568-bf37-37a30ff62899",
   "metadata": {
    "tags": []
   },
   "source": [
    "output = model(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81d35ba5-4b62-416c-9da4-1a2c32e3e972",
   "metadata": {},
   "source": [
    "out = output.logits.softmax(1).max(1).indices\n",
    "out"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9632faa-d93c-4880-a470-7e8208812bde",
   "metadata": {},
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7425a3bd-07ef-46b3-8d08-78aae36748a8",
   "metadata": {},
   "source": [
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "062fd15d-3a4c-4632-91c0-bf69eba02875",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2943b7c1-fa71-4f82-8ca1-6c72adf871cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46226bb6-076f-405d-be32-ac95acec3437",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a83c079-3983-4743-a20c-e7670b492805",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b29b88f5-ca7c-41b1-aa40-1a39f3f599fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my name\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98deed86-d2cd-4575-9171-66e91fb4089b",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my name is\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04ef0913-467f-4c87-a852-2899109f7dff",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my name is Bob\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc2a9138-0a82-4e47-8b9d-7fe5af5f9c63",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my name is Bob. What\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f2ea876-31fa-4833-9404-2eb1d7decf9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.decode(model(**tokenizer(\"Hello, my name is Bob. What is\", return_tensors=\"pt\")).logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1663f1b-4a85-427e-9074-9ca2c6b7887c",
   "metadata": {},
   "source": [
    "tokenizer(\"Hello, my name is Bob. What is\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fa7a71e-f999-466b-829b-a70eb016311f",
   "metadata": {
    "tags": []
   },
   "source": [
    "output = model(input_ids=tokenizer(\"Hello, my name is Bob. What is\", return_tensors=\"pt\")['input_ids'], \n",
    "                       labels=tokenizer(\", my name is Bob. What is your\", return_tensors=\"pt\")['input_ids'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "842384ab-c4b4-4a20-8894-571e2f1e1e17",
   "metadata": {},
   "source": [
    "tokenizer.decode(output.logits[0].softmax(1).max(1).indices)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d340ca8-2ac0-402f-bacc-40a13c108c61",
   "metadata": {
    "tags": []
   },
   "source": [
    "output.loss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c6bc580-225a-4c29-ae2f-749b95667aaf",
   "metadata": {},
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33bd630-803f-4f12-bc87-cddf66c7d2e7",
   "metadata": {},
   "source": [
    "## DEBUG: END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc3d07-780f-41a9-95c1-f44a06a1c8d4",
   "metadata": {},
   "source": [
    "### Training\n",
    "## [Fails to run on specificed GPU -- defaults to \"cuda:0\" -- due to a bug in `transformers`]\n",
    "At this point, only three steps remain:\n",
    "\n",
    "Define your training hyperparameters in TrainingArguments. The only required parameter is output_dir which specifies where to save your model. Youâ€™ll push this model to the Hub by setting push_to_hub=True (you need to be signed in to Hugging Face to upload your model).\n",
    "Pass the training arguments to Trainer along with the model, datasets, and data collator.\n",
    "Call train() to finetune your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26708f55-eb45-40d0-bf46-63d6822c61f6",
   "metadata": {},
   "source": [
    "### Disable external telemetry (this may not be reachable from the local network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31adc1d5-d287-4950-9a63-09598f5a235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf85403e-ac95-487a-b721-e17cd937078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901ab61b-c5dd-4096-b3fb-b69c1f74f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CUDA environment BEFORE importing torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{GPU}\"  # This shrinks the GPU universe and maps cuda:0 to {GPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddc80903-c9bb-4753-8e9a-04748ef058ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98364ad5-5c84-4c6c-9b2a-a44e5cde7848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device() # This really is device {GPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d516a66-8dd6-41ea-a4c2-c522a1fbf81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1fbe7cb-550c-480d-8ed7-30b210c6ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR=None\n",
    "if CHECKPOINT_DIR is not None:\n",
    "    model = model.from_pretrained(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dec0a2e4-8319-4f5a-9037-a5de216146ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, I'm writing a new language for you. But first, I'd like to tell you about the language itself\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'm trying to be as expressive as possible. In order to be expressive, it is necessary to know\"},\n",
       " {'generated_text': \"Hello, I'm a language model, so I don't get much of a license anymore, but I'm probably more familiar with other languages on that\"},\n",
       " {'generated_text': \"Hello, I'm a language model, a functional model... It's not me, it's me!\\n\\nI won't bore you with how\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I need to give language model a set of properties that\"}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77ff6093-944f-4ec1-8242-c9de25999d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: Dogs have been selectively bred by humans specifically to look different from each other.  People like variety, and exaggerate it in dogs.  People haven't been breeding killer whales to all look really different, so they pretty much all look similar (in a broad sense).Asteroid belts are not dense. This is a myth perpetuated by popular media. Space is so big that if you were flying through an asteroid belt, you would not come close to any asteroids unless you aimed very carefully right at one. Collisions between large asteroids occurs on the order of once every several million years. So no, the average mass density of\n",
      "inputs_0[:PRIME_LEN]: Dogs have \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Dogs have iced heads: a new study finds that nearly two-thirds think the dog breeds are less important to their health than others\\n\\n'},\n",
       " {'generated_text': 'Dogs have ËœËœËœËœËœËœËœ.\\n\\nSome pets have \"ËœËœËœËœËœËœËœ.\\n\\nSome pets'},\n",
       " {'generated_text': 'Dogs have ichthyogenic disease, which is a deadly disease affecting about 150,000 dogs. Dogs are one of the most popular pets for'},\n",
       " {'generated_text': \"Dogs have ursine amyloid receptors in their own cells and that doesn't stop a dog from taking their own urine, said Robert E\"},\n",
       " {'generated_text': 'Dogs have erythropoiesis â€“ which is a swelling of skin around or around your eyes. These are called anaphylaxis or'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 10\n",
    "input_ids_0 = torch.tensor(dataset_tokenized_batched[\"test\"][\"input_ids\"][0])\n",
    "inputs_0 = tokenizer.decode(input_ids_0)\n",
    "label_ids_0 = torch.tensor(dataset_tokenized_batched[\"test\"][\"labels\"][0])\n",
    "labels_0 = tokenizer.decode(label_ids_0)\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:PRIME_LEN]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6858aa5f-8804-46b6-a2ba-6ac0c0c315c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: I have to tell ya honestly\n",
      "inputs_0[:PRIME_LEN]: I have to tell ya honestly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I have to tell ya honestly, I've had it with everyone. I've had people think I'm an asshole because I'm an activist. It\"},\n",
       " {'generated_text': \"I have to tell ya honestly, I'd be happy to tell you how lucky I was to get married. I never expected to live happily ever after\"},\n",
       " {'generated_text': 'I have to tell ya honestly,\" said the manager, who told me his team can only take a third more point with four minutes on the clock,'},\n",
       " {'generated_text': 'I have to tell ya honestly I\\'m not really a big fan of this game\\'s story,\" he said in a phone interview from Paris before his team'},\n",
       " {'generated_text': 'I have to tell ya honestly, he had my money because I knew as well as I did he was going to be the star of that show.'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 30\n",
    "inputs_0 = \"I have to tell ya honestly\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:PRIME_LEN]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2223253-b63e-4ee6-af82-14b691f56cca",
   "metadata": {},
   "source": [
    "TrainingArguments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abe2c2aa-fcc9-4107-b216-10c54613ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "time = datetime.datetime.now().strftime('%H.%M')\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{MODEL_NAME}-{DATASET_NAME}/date={date}/time={time}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    #place_model_on_device=torch.device(f\"cuda:{GPU}\"),\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=4.0,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized_batched[\"train\"],\n",
    "    eval_dataset=dataset_tokenized_batched[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3115025c-743c-4cdf-bda6-0b004fa6bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dkarpeyev/.conda/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 267621\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16728\n",
      "  Number of trainable parameters = 124439808\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16728' max='16728' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16728/16728 1:00:37, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.422500</td>\n",
       "      <td>3.335937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.375500</td>\n",
       "      <td>3.308554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.350300</td>\n",
       "      <td>3.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.336400</td>\n",
       "      <td>3.292390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-1000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-1000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-1000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-1500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-1500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-1500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-2000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-2000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-2000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-2500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-2500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-2500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-3000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-3000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-3000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-3500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-3500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-3500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-4000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-4000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-4000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-4000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66508\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-4500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-4500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-4500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-5000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-5000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-5000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-5500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-5500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-5500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-6000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-6000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-6000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-6500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-6500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-6500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-7000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-7000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-7000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-7500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-7500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-7500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-8000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-8000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-8000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-8000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66508\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-8500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-8500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-8500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-9000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-9000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-9000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-9500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-9500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-9500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-10000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-10000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-10000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-10500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-10500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-10500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-11000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-11000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-11000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-11500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-11500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-11500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-12000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-12000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-12000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-12500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-12500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-12500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-12500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66508\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-13000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-13000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-13000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-13500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-13500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-13500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-14000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-14000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-14000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-14500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-14500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-14500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-15000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-15000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-15000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-15500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-15500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-15500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-16000\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-16000/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-16000/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-16500\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-16500/config.json\n",
      "Configuration saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-16500/generation_config.json\n",
      "Model weights saved in gpt2-eli5/date=2023-02-09/time=21.39/checkpoint-16500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66508\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16728, training_loss=3.3876338978807987, metrics={'train_runtime': 3637.9317, 'train_samples_per_second': 294.256, 'train_steps_per_second': 4.598, 'total_flos': 6.9927234895872e+16, 'train_loss': 3.3876338978807987, 'epoch': 4.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "460f9f85-54d6-4eae-a352-18940ab456d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: Dogs have been selectively bred by humans specifically to look different from each other.  People like variety, and exaggerate it in dogs.  People haven't been breeding killer whales to all look really different, so they pretty much all look similar (in a broad sense).Asteroid belts are not dense. This is a myth perpetuated by popular media. Space is so big that if you were flying through an asteroid belt, you would not come close to any asteroids unless you aimed very carefully right at one. Collisions between large asteroids occurs on the order of once every several million years. So no, the average mass density of\n",
      "inputs_0[:PRIME_LEN]: Dogs have \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Dogs have  higher mortality because they spend much more time at night and are better protected by their tails because they live longer and also are more doc'},\n",
       " {'generated_text': 'Dogs have ~~one~~ brain. Dogs and cats seem to have the same brain. This means they have similar parts. So there are '},\n",
       " {'generated_text': 'Dogs have  < 20% of their body weight stored for as long as necessary. But more importantly, there is evidence that these cats are often'},\n",
       " {'generated_text': \"Dogs have  a more variable sense of smell than their fur, and that doesn't mean they won't smell their way through it. \\n\"},\n",
       " {'generated_text': 'Dogs have erythrocyte pigment receptors which allow for their red blood cells to become sensitive to other pigments. This means they are capable'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 10\n",
    "input_ids_0 = torch.tensor(dataset_tokenized_batched[\"test\"][\"input_ids\"][0])\n",
    "inputs_0 = tokenizer.decode(input_ids_0)\n",
    "label_ids_0 = torch.tensor(dataset_tokenized_batched[\"test\"][\"labels\"][0])\n",
    "labels_0 = tokenizer.decode(label_ids_0)\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:PRIME_LEN]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd10a728-bdd4-4b3a-b8db-4b6753eb7431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: I have to tell ya honestly\n",
      "inputs_0[:PRIME_LEN]: I have to tell ya honestly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I have to tell ya honestly, I've had it with everyone. I've had people think I'm an asshole because I'm an activist. It\"},\n",
       " {'generated_text': \"I have to tell ya honestly, I'd be happy to tell you how lucky I was to get married. I never expected to live happily ever after\"},\n",
       " {'generated_text': 'I have to tell ya honestly,\" said the manager, who told me his team can only take a third more point with four minutes on the clock,'},\n",
       " {'generated_text': 'I have to tell ya honestly I\\'m not really a big fan of this game\\'s story,\" he said in a phone interview from Paris before his team'},\n",
       " {'generated_text': 'I have to tell ya honestly, he had my money because I knew as well as I did he was going to be the star of that show.'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 30\n",
    "inputs_0 = \"I have to tell ya honestly\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:PRIME_LEN]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2747eefb-781a-48cf-9da2-8c7acce489dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: Please explain AI to me.\n",
      "inputs_0[:PRIME_LEN]: Please explain AI to me.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Please explain AI to me. I can do a deep learning algorithm that can perform a lot better under pressure (like a robot) than a human can'},\n",
       " {'generated_text': 'Please explain AI to me. What you are trying to do is to create a human-language program. This software will be similar to what a program'},\n",
       " {'generated_text': 'Please explain AI to me.\\n\\nWhen talking about how computers play chess, one of the biggest concerns is speed at which they can do these comput'},\n",
       " {'generated_text': 'Please explain AI to me. I will explain AI to you in 4 steps.\\nA computer is programmed in many different ways. It is programmed to'},\n",
       " {'generated_text': \"Please explain AI to me.  If you can't solve the problem as you are talking about, then your brain is really not good at solving it\"}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 3000\n",
    "inputs_0 = \"Please explain AI to me.\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:PRIME_LEN]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5384425-2890-46f3-944c-9d32488859df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: Please explain AI to me.\n",
      "inputs_0[:PRIME_LEN]: Please explain AI to me.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Please explain AI to me. I can do a deep learning algorithm that can perform a lot better under pressure (like a robot) than a human can'},\n",
       " {'generated_text': 'Please explain AI to me. What you are trying to do is to create a human-language program. This software will be similar to what a program'},\n",
       " {'generated_text': 'Please explain AI to me.\\n\\nWhen talking about how computers play chess, one of the biggest concerns is speed at which they can do these comput'},\n",
       " {'generated_text': 'Please explain AI to me. I will explain AI to you in 4 steps.\\nA computer is programmed in many different ways. It is programmed to'},\n",
       " {'generated_text': \"Please explain AI to me.  If you can't solve the problem as you are talking about, then your brain is really not good at solving it\"}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 3000\n",
    "inputs_0 = \"Please explain AI to me.\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:{PRIME_LEN}]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47b48c96-a508-4b69-84b6-41ee717816fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_0: Russia has invaded\n",
      "inputs_0[:PRIME_LEN]: Russia has invaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Russia has invaded Russia and killed us.\\n\\nIf you are coming from a side, there are two things you should focus on:\\n\\na'},\n",
       " {'generated_text': 'Russia has invaded Syria in the past two days, and has killed or driven hundreds of thousands of people, as has been pointed out, with artillery fire'},\n",
       " {'generated_text': \"Russia has invaded the Arctic?** Well, actually it hasn't invaded Russia.** The first major military strike over Russia happened a month ago as part\"},\n",
       " {'generated_text': 'Russia has invaded Afghanistan in the 19th century, or at least has launched major missile strikes against American forces operating in the region in the last decades.'},\n",
       " {'generated_text': \"Russia has invaded, it's the same reason why Germany was attacked.\\n\\n**Now**: The most dramatic thing to happen is the current geopolitical\"}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRIME_LEN = 3000\n",
    "inputs_0 = \"Russia has invaded\"\n",
    "\n",
    "print(f\"inputs_0: {inputs_0}\")\n",
    "print(f\"inputs_0[:PRIME_LEN]: {inputs_0[:PRIME_LEN]}\")\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(41)\n",
    "generator(inputs_0[:PRIME_LEN], max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282c0f6-f00a-4f7f-841e-2a62cd00e9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
